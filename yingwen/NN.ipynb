{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5eed5f1-8138-453d-8e6a-5d55ccd5559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read data from excel \n",
    "data = pd.read_excel('Dry_Bean_Dataset.xlsx')\n",
    "\n",
    "X = data.drop(columns = ['Class'])\n",
    "y = data['Class']\n",
    "\n",
    "# Use mapping for the labels\n",
    "# y = y.map({'SEKER': 0, 'BARBUNYA': 1, 'BOMBAY': 2, 'CALI': 3, 'HOROZ': 4, 'SIRA': 5, 'DERMASON': 6})\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"特征数据类型:\\n\", X.dtypes)  # 确保所有特征是数值型\n",
    "# print(\"标签分布:\\n\", y.value_counts())  # 检查类别平衡性\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bb0f73f-f010-4d64-8f52-72ded7997e86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MLPClassifier in module sklearn.neural_network._multilayer_perceptron:\n",
      "\n",
      "class MLPClassifier(sklearn.base.ClassifierMixin, BaseMultilayerPerceptron)\n",
      " |  MLPClassifier(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |  \n",
      " |  Multi-layer Perceptron classifier.\n",
      " |  \n",
      " |  This model optimizes the log-loss function using LBFGS or stochastic\n",
      " |  gradient descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : array-like of shape(n_layers - 2,), default=(100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
      " |        by Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      Strength of the L2 regularization term. The L2 regularization term\n",
      " |      is divided by the sample size when added to the loss.\n",
      " |  \n",
      " |  batch_size : int, default='auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`.\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate at each\n",
      " |        time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when ``solver='sgd'``.\n",
      " |  \n",
      " |  learning_rate_init : float, default=0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : float, default=0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, default=200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Determines random number generation for weights and bias\n",
      " |      initialization, train-test split if early stopping is used, and batch\n",
      " |      sampling when solver='sgd' or 'adam'.\n",
      " |      Pass an int for reproducible results across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
      " |      unless ``learning_rate`` is set to 'adaptive', convergence is\n",
      " |      considered to be reached and training stops.\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  momentum : float, default=0.9\n",
      " |      Momentum for gradient descent update. Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : bool, default=True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least tol for\n",
      " |      ``n_iter_no_change`` consecutive epochs. The split is stratified,\n",
      " |      except in a multilabel setting.\n",
      " |      If early stopping is False, then the training stops when the training\n",
      " |      loss does not improve by more than tol for n_iter_no_change consecutive\n",
      " |      passes over the training set.\n",
      " |      Only effective when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True.\n",
      " |  \n",
      " |  beta_1 : float, default=0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'.\n",
      " |  \n",
      " |  beta_2 : float, default=0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'.\n",
      " |  \n",
      " |  epsilon : float, default=1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'.\n",
      " |  \n",
      " |  n_iter_no_change : int, default=10\n",
      " |      Maximum number of epochs to not meet ``tol`` improvement.\n",
      " |      Only effective when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  max_fun : int, default=15000\n",
      " |      Only used when solver='lbfgs'. Maximum number of loss function calls.\n",
      " |      The solver iterates until convergence (determined by 'tol'), number\n",
      " |      of iterations reaches max_iter, or this number of loss function calls.\n",
      " |      Note that number of loss function calls will be greater than or equal\n",
      " |      to the number of iterations for the `MLPClassifier`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray or list of ndarray of shape (n_classes,)\n",
      " |      Class labels for each output.\n",
      " |  \n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  best_loss_ : float or None\n",
      " |      The minimum loss reached by the solver throughout fitting.\n",
      " |      If `early_stopping=True`, this attribute is set to `None`. Refer to\n",
      " |      the `best_validation_score_` fitted attribute instead.\n",
      " |  \n",
      " |  loss_curve_ : list of shape (`n_iter_`,)\n",
      " |      The ith element in the list represents the loss at the ith iteration.\n",
      " |  \n",
      " |  validation_scores_ : list of shape (`n_iter_`,) or None\n",
      " |      The score at each iteration on a held-out validation set. The score\n",
      " |      reported is the accuracy score. Only available if `early_stopping=True`,\n",
      " |      otherwise the attribute is set to `None`.\n",
      " |  \n",
      " |  best_validation_score_ : float or None\n",
      " |      The best validation score (i.e. accuracy score) that triggered the\n",
      " |      early stopping. Only available if `early_stopping=True`, otherwise the\n",
      " |      attribute is set to `None`.\n",
      " |  \n",
      " |  t_ : int\n",
      " |      The number of training samples seen by the solver during fitting.\n",
      " |  \n",
      " |  coefs_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The number of iterations the solver has run.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : str\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  MLPRegressor : Multi-layer Perceptron regressor.\n",
      " |  BernoulliRBM : Bernoulli Restricted Boltzmann Machine (RBM).\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPClassifier trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense numpy arrays or\n",
      " |  sparse scipy arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E. \"Connectionist learning procedures.\"\n",
      " |  Artificial intelligence 40.1 (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio.\n",
      " |  \"Understanding the difficulty of training deep feedforward neural networks.\"\n",
      " |  International Conference on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  :arxiv:`He, Kaiming, et al (2015). \"Delving deep into rectifiers:\n",
      " |  Surpassing human-level performance on imagenet classification.\" <1502.01852>`\n",
      " |  \n",
      " |  :arxiv:`Kingma, Diederik, and Jimmy Ba (2014)\n",
      " |  \"Adam: A method for stochastic optimization.\" <1412.6980>`\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.neural_network import MLPClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> from sklearn.model_selection import train_test_split\n",
      " |  >>> X, y = make_classification(n_samples=100, random_state=1)\n",
      " |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
      " |  ...                                                     random_state=1)\n",
      " |  >>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
      " |  >>> clf.predict_proba(X_test[:1])\n",
      " |  array([[0.038..., 0.961...]])\n",
      " |  >>> clf.predict(X_test[:5, :])\n",
      " |  array([1, 0, 1, 0, 1])\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.8...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseMultilayerPerceptron\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None)\n",
      " |      Update the model with a single iteration over the given data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      classes : array of shape (n_classes,), default=None\n",
      " |          Classes across all calls to partial_fit.\n",
      " |          Can be obtained via `np.unique(y_all)`, where y_all is the\n",
      " |          target vector of the entire dataset.\n",
      " |          This argument is required for the first call to partial_fit\n",
      " |          and can be omitted in the subsequent calls.\n",
      " |          Note that y doesn't need to contain all labels in `classes`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Trained MLP model.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron classifier.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray, shape (n_samples,) or (n_samples, n_classes)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return the log of probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_y_prob : ndarray of shape (n_samples, n_classes)\n",
      " |          The predicted log-probability of the sample for each class\n",
      " |          in the model, where classes are ordered as they are in\n",
      " |          `self.classes_`. Equivalent to `log(predict_proba(X))`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_prob : ndarray of shape (n_samples, n_classes)\n",
      " |          The predicted probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in `self.classes_`.\n",
      " |  \n",
      " |  set_partial_fit_request(self: sklearn.neural_network._multilayer_perceptron.MLPClassifier, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neural_network._multilayer_perceptron.MLPClassifier\n",
      " |      Request metadata passed to the ``partial_fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      classes : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``classes`` parameter in ``partial_fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.neural_network._multilayer_perceptron.MLPClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neural_network._multilayer_perceptron.MLPClassifier\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray or sparse matrix of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from abc.ABCMeta\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa3b52b-6fe7-47ab-bd6a-6035fdb1404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14198\\anaconda3\\envs\\Yingwen\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# split the training data and testing data, then do standarization\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, shuffle=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# construct the multi-layer perceptron model\n",
    "# alpha: regularization parameter, solver: optimization algorithm, tol: stop condition\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=100, alpha = 1e-4,\n",
    "                        solver='lbfgs', tol=1e-6, random_state=42)\n",
    "\n",
    "mlp_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred = mlp_clf.predict(X_train_scaled)\n",
    "y_test_pred = mlp_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29788370-7091-4bba-af99-c631ff5fc3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9413\n",
      "Testing accuracy: 0.9302\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = accuracy_score(y_train_pred, y_train)\n",
    "accuracy_test = accuracy_score(y_test_pred, y_test)\n",
    "\n",
    "print(f\"Training accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Testing accuracy: {accuracy_test:.4f}\")\n",
    "\n",
    "# Accuracy score\n",
    "#Training accuracy: 0.9413\n",
    "#Testing accuracy: 0.9302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1572535c-2d74-4e4a-9864-bca5a9aaa765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Evaluation Metrics:\n",
      "Confusion Matrix:\n",
      "+------+------+-----+------+------+------+------+\n",
      "|    0 |    1 |   2 |    3 |    4 |    5 |    6 |\n",
      "+======+======+=====+======+======+======+======+\n",
      "| 1550 |    4 |   0 |    1 |    0 |   32 |   27 |\n",
      "+------+------+-----+------+------+------+------+\n",
      "|    4 | 1007 |   0 |   31 |    4 |   14 |    1 |\n",
      "+------+------+-----+------+------+------+------+\n",
      "|    0 |    0 | 405 |    0 |    0 |    0 |    0 |\n",
      "+------+------+-----+------+------+------+------+\n",
      "|    3 |   26 |   0 | 1262 |   13 |    9 |    0 |\n",
      "+------+------+-----+------+------+------+------+\n",
      "|    1 |    2 |   0 |   20 | 1452 |   32 |   13 |\n",
      "+------+------+-----+------+------+------+------+\n",
      "|   23 |    6 |   0 |    4 |   22 | 1863 |  182 |\n",
      "+------+------+-----+------+------+------+------+\n",
      "|   33 |    0 |   0 |    0 |    3 |  144 | 2695 |\n",
      "+------+------+-----+------+------+------+------+\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1614\n",
      "           1       0.96      0.95      0.96      1061\n",
      "           2       1.00      1.00      1.00       405\n",
      "           3       0.96      0.96      0.96      1313\n",
      "           4       0.97      0.96      0.96      1520\n",
      "           5       0.89      0.89      0.89      2100\n",
      "           6       0.92      0.94      0.93      2875\n",
      "\n",
      "    accuracy                           0.94     10888\n",
      "   macro avg       0.95      0.95      0.95     10888\n",
      "weighted avg       0.94      0.94      0.94     10888\n",
      "\n",
      "MLP Evaluation Metrics:\n",
      "Confusion Matrix:\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   0 |   1 |   2 |   3 |   4 |   5 |   6 |\n",
      "+=====+=====+=====+=====+=====+=====+=====+\n",
      "| 394 |   4 |   0 |   0 |   0 |   9 |   6 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   2 | 242 |   0 |  12 |   0 |   5 |   0 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   0 |   0 | 117 |   0 |   0 |   0 |   0 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   1 |  10 |   0 | 300 |   5 |   1 |   0 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   0 |   3 |   0 |   3 | 387 |   9 |   6 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   4 |   1 |   0 |   0 |   8 | 474 |  49 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   3 |   0 |   0 |   0 |   1 |  44 | 623 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       413\n",
      "           1       0.93      0.93      0.93       261\n",
      "           2       1.00      1.00      1.00       117\n",
      "           3       0.95      0.95      0.95       317\n",
      "           4       0.97      0.95      0.96       408\n",
      "           5       0.87      0.88      0.88       536\n",
      "           6       0.91      0.93      0.92       671\n",
      "\n",
      "    accuracy                           0.93      2723\n",
      "   macro avg       0.94      0.94      0.94      2723\n",
      "weighted avg       0.93      0.93      0.93      2723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tabulate import tabulate\n",
    "\n",
    "model_evaluation_metrics1 = {}\n",
    "\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "classif_report_train = classification_report(y_train, y_train_pred)\n",
    "\n",
    "model_evaluation_metrics1 = {\n",
    "    'Confusion Matrix': conf_matrix_train,\n",
    "    'Classification Report': classif_report_train\n",
    "}\n",
    "\n",
    "print(f\"MLP Evaluation Metrics:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(tabulate(conf_matrix_train, headers=mlp_clf.classes_, tablefmt='grid'))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classif_report_train)\n",
    "\n",
    "model_evaluation_metrics2 = {}\n",
    "\n",
    "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "classif_report_test = classification_report(y_test, y_test_pred)\n",
    "\n",
    "model_evaluation_metrics2 = {\n",
    "    'Confusion Matrix': conf_matrix_test,\n",
    "    'Classification Report': classif_report_test\n",
    "}\n",
    "\n",
    "print(f\"MLP Evaluation Metrics:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(tabulate(conf_matrix_test, headers=mlp_clf.classes_, tablefmt='grid'))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classif_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24530fe9-cd80-45a4-b86c-75513a646ce8",
   "metadata": {},
   "source": [
    "接下来可以尝试\n",
    "1. 增加1，5，6类（错误率较高）的数据\n",
    "2. 交叉验证调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0a2722f-e4fa-4b5a-bdbf-9dcc21bf7f24",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 20), 'max_iter': 200, 'solver': 'lbfgs'}\n",
      "Best CV accuracy: 0.9294638032726311\n",
      "Test set accuracy: 0.9327947117150202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14198\\anaconda3\\envs\\yingwen\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# First try cross-validation \n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "mlp_clf2 = MLPClassifier(early_stopping=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes':[(50,), (50, 20), (100,)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'max_iter': [200, 500],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(\n",
    "    estimator=mlp_clf2,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',  # 可根据需求改为 'f1_macro' 等\n",
    "    n_jobs=-1,           # 并行计算\n",
    "    verbose=1            # 打印进度\n",
    ")\n",
    "\n",
    "# 在标准化后的训练集上搜索\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 输出最佳参数和交叉验证分数\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", grid.best_score_)\n",
    "\n",
    "# 在测试集上评估\n",
    "best_mlp = grid.best_estimator_\n",
    "test_score = best_mlp.score(X_test_scaled, y_test)\n",
    "print(\"Test set accuracy:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "049c0720-1662-4a52-b013-28d94ddc2ad9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Evaluation Metrics:\n",
      "Confusion Matrix:\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   0 |   1 |   2 |   3 |   4 |   5 |   6 |\n",
      "+=====+=====+=====+=====+=====+=====+=====+\n",
      "| 392 |   2 |   0 |   1 |   0 |   8 |  10 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   1 | 242 |   0 |  11 |   0 |   7 |   0 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   0 |   0 | 117 |   0 |   0 |   0 |   0 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   1 |  11 |   0 | 300 |   4 |   1 |   0 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   0 |   4 |   0 |   5 | 386 |   8 |   5 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   4 |   1 |   0 |   0 |   6 | 479 |  46 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   5 |   0 |   0 |   0 |   1 |  41 | 624 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       413\n",
      "           1       0.93      0.93      0.93       261\n",
      "           2       1.00      1.00      1.00       117\n",
      "           3       0.95      0.95      0.95       317\n",
      "           4       0.97      0.95      0.96       408\n",
      "           5       0.88      0.89      0.89       536\n",
      "           6       0.91      0.93      0.92       671\n",
      "\n",
      "    accuracy                           0.93      2723\n",
      "   macro avg       0.94      0.94      0.94      2723\n",
      "weighted avg       0.93      0.93      0.93      2723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = best_mlp.predict(X_test_scaled)\n",
    "\n",
    "model_evaluation_metrics3 = {}\n",
    "\n",
    "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "classif_report_test = classification_report(y_test, y_test_pred)\n",
    "\n",
    "model_evaluation_metrics3 = {\n",
    "    'Confusion Matrix': conf_matrix_test,\n",
    "    'Classification Report': classif_report_test\n",
    "}\n",
    "\n",
    "print(f\"MLP Evaluation Metrics:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(tabulate(conf_matrix_test, headers=mlp_clf.classes_, tablefmt='grid'))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classif_report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626e186-1352-4813-9c25-2b7c12c66951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
